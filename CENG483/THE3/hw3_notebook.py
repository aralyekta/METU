# -*- coding: utf-8 -*-
"""HW3-Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E7SzDx7RdDFTk-n3ssjksexcSUtn22TG
"""

from google.colab import drive
import re
drive.mount('/content/drive')

!cp -r /content/drive/MyDrive/Vision-HW3/* .
!cp -r vision-hw3/* .
!cp -r src/* .
!tar -xvf the3_data.tar.gz

# ---- hyper-parameters ----
# You should tune these hyper-parameters using:
# (i) your reasoning and observations, 
# (ii) by tuning it on the validation set, using the techniques discussed in class.
# You definitely can add more hyper-parameters here.

# --- imports ---
import random
import torch
import os
import matplotlib.pyplot as plt
import numpy as np
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
import hw3utils
torch.multiprocessing.set_start_method('spawn', force=True)
# ---- utility functions -----
def get_loaders(batch_size,device):
    data_root = 'ceng483-f22-hw3-dataset' 
    train_set = hw3utils.HW3ImageFolder(root=os.path.join(data_root,'train'),device=device)
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)
    val_set = hw3utils.HW3ImageFolder(root=os.path.join(data_root,'val'),device=device)
    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)
    # Note: you may later add test_loader to here.
    return train_loader, val_loader

# ---- ConvNet -----
class Net(nn.Module):
    def __init__(self, n_layers, n_kernels):
        super(Net, self).__init__()
        self.n_layers = n_layers
        self.n_kernels = n_kernels

        if n_layers == 1:
          self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, padding=1)
        elif n_layers == 2:
          self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_kernels, kernel_size=3, padding=1)
          self.conv2 = nn.Conv2d(in_channels=n_kernels, out_channels=3, kernel_size=3, padding=1)
        elif n_layers == 4:
          self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_kernels, kernel_size=3, padding=1)
          self.conv2 = nn.Conv2d(in_channels=n_kernels, out_channels=1, kernel_size=3, padding=1)
          self.conv3 = nn.Conv2d(in_channels=1, out_channels=n_kernels, kernel_size=3, padding=1)
          self.conv4 = nn.Conv2d(in_channels=n_kernels, out_channels=3, kernel_size=3, padding=1)
        
        self.relu = nn.ReLU()

    def forward(self, grayscale_image):
        # apply your network's layers in the following lines:
        if self.n_layers == 1:
          x = self.conv1(grayscale_image)
        
        elif self.n_layers == 2:
          x = self.relu(self.conv1(grayscale_image))
          x = self.conv2(x)

        elif self.n_layers == 4:
          x = self.relu(self.conv1(grayscale_image))
          x = self.relu(self.conv2(x))
          x = self.relu(self.conv3(x))
          x = self.conv4(x)
        
        return x

# seeding for reproducibility
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)

def get_orig_range(arr):
  return ((arr - (-1)) * (1/2 * 255)).long()

def main(config_cnt=1):

  # Early stop patience parameter, if no improvement of delta
  # in last *patience* iterations, early stop the training...
  patience = 5
  delta = 1e-3

  # parameters as stated in the homework text
  batch_size = 16
  max_num_epoch = 100
  hps = {'lr':[1e-1, 1e-2, 1e-3, 1e-4], 'kernel_cnt':[2,4,8], 'n_layers':[1,2,4]}

  # ---- options ---- as provided
  DEVICE_ID = 'cuda:0' 
  LOG_DIR = 'checkpoints'
  IMG_DIR = 'img'
  VISUALIZE = False # set True to visualize input, prediction and the output from the last batch
  LOAD_CHKPT = False

  run_cnt = 0 # how many configs have been tried
  best_val_acc = 0 # holds best 12-margin validation accuracy
  best_config = "" # holds the config achieving best_val_acc

  # loop until desired amount of configs have been tried
  while(run_cnt < config_cnt):

    # randomly pick one from each parameter type
    lr = random.sample(hps['lr'], 1)[0]
    kc = random.sample(hps['kernel_cnt'], 1)[0]
    num_l = random.sample(hps['n_layers'], 1)[0]
    
    run_cnt += 1
    # ---- training code -----
    device = torch.device(DEVICE_ID)
    print('device: ' + str(device))
    net = Net(num_l, kc).to(device=device) # initialize network with parameters
    criterion = nn.MSELoss()
    optimizer = optim.SGD(net.parameters(), lr=lr)
    train_loader, val_loader = get_loaders(batch_size,device)

    if LOAD_CHKPT:
      print('loading the model from the checkpoint')
      net.load_state_dict(os.path.join(LOG_DIR,'checkpoint.pt'))
    
    min_val_loss = np.iinfo('uint32').max
    curr_patience = 0

    for epoch in range(max_num_epoch):  

      if curr_patience >= patience:
        print(f"No significant improvement since last {patience} iterations, early stopping at epoch: {epoch+1}...")
        break
      
      running_loss = 0.0 # training loss of the network
      train_total = 0
      for iteri, data in enumerate(train_loader, 0):
        inputs, targets = data
        optimizer.zero_grad()
        preds = net(inputs)
        loss = criterion(preds, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        train_total += inputs.shape[0]
      
      # validation run at each epoch
      net.eval()
      val_loss = 0.0
      val_total = 0
      cur_acc = 0.0
      with torch.no_grad():
        for iteri, data in enumerate(val_loader, 0):
          val_inputs, val_targets = data
          val_preds = net(val_inputs)
          _loss = criterion(val_preds, val_targets)
          val_loss += _loss.item()
          val_total += val_inputs.shape[0]

          # convert prediction and label to its original range
          _val_preds = get_orig_range(val_preds)
          _val_targets = get_orig_range(val_targets)
          cur_acc += (torch.abs(_val_targets - _val_preds) < 12).sum()
      
      cur_acc /= (val_total*80*80*3)
      
      # if at least delta amount of improvement compared
      # to the current best, then set run patience to zero
      if (val_loss / val_total + delta) < min_val_loss:
        min_val_loss = val_loss / val_total
        curr_patience = 0
      else:
        curr_patience += 1
      
      # update the best accuracy if current config is better
      if cur_acc >= best_val_acc:
        best_val_acc = cur_acc
        best_config = f"LR{lr}_KernelCount{kc}_ConvLayerCount_{num_l}_Epoch{epoch}"

      print(f'Epoch: {epoch+1}, Train Loss: {running_loss / train_total}, Valid Loss: {val_loss / val_total}, 12-Margin Accuracy: {cur_acc}')
      print('Saving the model, end of epoch %d' % (epoch+1))
      if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)
      torch.save(net.state_dict(), os.path.join(LOG_DIR,'checkpoint.pt'))
      hw3utils.visualize_batch(val_inputs,val_preds,val_targets,os.path.join(IMG_DIR,f'LR{lr}_KernelCount{kc}_ConvLayerCount_{num_l}_Epoch{epoch+1}.png'))
    print(f'Finished Training Config: LR: {lr}, Kernel Count: {kc}, Conv. Layer Count: {num_l}')

  print(f"Best config {best_config} reached 12-margin accuracy of: {best_val_acc}")

# out of 36 possible configs, we are
# being generous and trying out 15

if __name__ == "__main__":
  main(config_cnt=15)

# ---- FurtherNet -----
class FurtherNet(nn.Module):
    def __init__(self, batchnorm=False, hyptan=False, sixteen=False, separable=False, group=False):
        super(FurtherNet, self).__init__()

        # parameters from the best config as stated above....
        self.n_layers = 2
        self.n_kernels = 4
        self.batchnorm = batchnorm
        self.hyptan = hyptan
        self.sixteen = sixteen
        self.separable = separable
        self.group = group

        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.n_kernels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=self.n_kernels, out_channels=3, kernel_size=3, padding=1)
        
        self.conv1_sixt = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.conv2_sixt = nn.Conv2d(in_channels=16, out_channels=3, kernel_size=3, padding=1)

        # out_channels=1*16 since 16 showed improvement
        self.sep_dep1 = nn.Conv2d(in_channels=1, out_channels=1*16, kernel_size=3, padding=1, groups=1)
        self.sep_point1 = nn.Conv2d(in_channels=1*16, out_channels=3, kernel_size=1)

        # out_channels=3*16 since 16 showed improvement and
        # both in_channels and out_channels must be divisible
        # by number of groups, which can only be 3 other than
        # trivial 1...
        self.group_conv1 = nn.Conv2d(in_channels=1, out_channels=3*16, kernel_size=3, padding=1, groups=1)
        self.group_conv2 = nn.Conv2d(in_channels=3*16, out_channels=3, kernel_size=3, padding=1, groups=3)
        
        self.bn = nn.BatchNorm2d(num_features=self.n_kernels)
        self.bn_sixt = nn.BatchNorm2d(num_features=16)
        self.bn_group = nn.BatchNorm2d(num_features=3*16)
        self.tanh = torch.tanh
        self.relu = nn.ReLU()

    def forward(self, grayscale_image):
        # apply your network's layers in the following lines:

        # trying batchnormalization...
        if self.batchnorm and not self.sixteen and not self.separable and not self.group:
          x = self.relu(self.bn(self.conv1(grayscale_image)))
          x = self.conv2(x)
        
        # trying 16 channels option...
        elif self.sixteen and not self.separable and not self.group:
          if self.batchnorm:
            x = self.relu(self.bn_sixt(self.conv1_sixt(grayscale_image)))
            x = self.conv2_sixt(x)
          else:
            x = self.relu(self.conv1_sixt(grayscale_image))
            x = self.conv2_sixt(x)

        # trying separable...
        # no need to consider an additional case for no sixteen since it improved...
        elif self.separable and not self.group:
          if self.batchnorm:
            x = self.relu(self.bn_sixt(self.sep_dep1(grayscale_image)))
            x = self.sep_point1(x)
          else:
            x = self.relu(self.sep_dep1(grayscale_image))
            x = self.sep_point1(x)

        # trying group conv...
        # no need to consider an additional case for no sixteen since it improved...
        elif self.group:
          if self.batchnorm:
            x = self.relu(self.bn_group(self.group_conv1(grayscale_image)))
            x = self.group_conv2(x)
          else:
            x = self.relu(self.group_conv1(grayscale_image))
            x = self.group_conv2(x)
        
        # regular case...
        else:
          x = self.relu(self.conv1(grayscale_image))
          x = self.conv2(x)
        
        # apply hyperbolic tangent at the end if true
        if self.hyptan:
          x = self.tanh(x)
        
        return x

def further_main(config_cnt=1, batchnorm=False, hyptan=False, sixteen=False, separable=False, group=False):

  # Early stop patience parameter, if no improvement of delta
  # in last *patience* iterations, early stop the training...
  # increased patience as this is the best config
  patience = 15
  delta = 1e-3

  # parameters as stated in the homework text, only the best LR this time
  batch_size = 16
  max_num_epoch = 100
  hps = {'lr':[1e-1]}

  # ---- options ---- as provided
  DEVICE_ID = 'cuda:0' 
  LOG_DIR = 'checkpoints'
  IMG_DIR = 'img'
  VISUALIZE = False # set True to visualize input, prediction and the output from the last batch
  LOAD_CHKPT = False

  run_cnt = 0 # how many configs have been tried
  best_val_acc = 0 # holds best 12-margin validation accuracy
  best_config = "" # holds the config achieving best_val_acc

  # loop until desired amount of configs have been tried
  while(run_cnt < config_cnt):

    # can only pick the best lr as given above...
    lr = random.sample(hps['lr'], 1)[0]
    
    run_cnt += 1
    # ---- training code -----
    device = torch.device(DEVICE_ID)
    print('device: ' + str(device))
    net = FurtherNet(batchnorm=batchnorm, hyptan=hyptan, sixteen=sixteen, separable=separable, group=group).to(device=device) # initialize network with parameters
    criterion = nn.MSELoss()
    optimizer = optim.SGD(net.parameters(), lr=lr)
    train_loader, val_loader = get_loaders(batch_size,device)

    if LOAD_CHKPT:
      print('loading the model from the checkpoint')
      net.load_state_dict(os.path.join(LOG_DIR,'checkpoint.pt'))
    
    min_val_loss = np.iinfo('uint32').max
    curr_patience = 0

    for epoch in range(max_num_epoch):  

      if curr_patience >= patience:
        print(f"No significant improvement since last {patience} iterations, early stopping at epoch: {epoch+1}...")
        break
      
      running_loss = 0.0 # training loss of the network
      train_total = 0
      for iteri, data in enumerate(train_loader, 0):
        inputs, targets = data
        optimizer.zero_grad()
        preds = net(inputs)
        loss = criterion(preds, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        train_total += inputs.shape[0]
      
      # validation run at each epoch
      net.eval()
      val_loss = 0.0
      val_total = 0
      cur_acc = 0.0
      with torch.no_grad():
        for iteri, data in enumerate(val_loader, 0):
          val_inputs, val_targets = data
          val_preds = net(val_inputs)
          _loss = criterion(val_preds, val_targets)
          val_loss += _loss.item()
          val_total += val_inputs.shape[0]

          # convert prediction and label to its original range
          _val_preds = get_orig_range(val_preds)
          _val_targets = get_orig_range(val_targets)
          cur_acc += (torch.abs(_val_targets - _val_preds) < 12).sum()
      
      cur_acc /= (val_total*80*80*3)
      
      # if at least delta amount of improvement compared
      # to the current best, then set run patience to zero
      if (val_loss / val_total + delta) < min_val_loss:
        min_val_loss = val_loss / val_total
        curr_patience = 0
      else:
        curr_patience += 1
      
      # update the best accuracy if current config is better
      # batchnorm=False, hyptan=False, sixteen=False, separable=False, group=False
      if cur_acc >= best_val_acc:
        best_val_acc = cur_acc
        best_config = f"BN:{batchnorm}_Tanh:{hyptan}_isSixteen:{sixteen}_Separable:{separable}_Group:{group}_Epoch{epoch+1}"

      print(f'Epoch: {epoch+1}, Train Loss: {running_loss / train_total}, Valid Loss: {val_loss / val_total}, 12-Margin Accuracy: {cur_acc}')
      print('Saving the model, end of epoch %d' % (epoch+1))
      if not os.path.exists(LOG_DIR):
        os.makedirs(LOG_DIR)
      torch.save(net.state_dict(), os.path.join(LOG_DIR,'checkpoint.pt'))
      hw3utils.visualize_batch(val_inputs,val_preds,val_targets,os.path.join(IMG_DIR,f'BN:{batchnorm}_Tanh:{hyptan}_isSixteen:{sixteen}_Separable:{separable}_Group:{group}_Epoch{epoch+1}.png'))
    print(f'f"BN:{batchnorm}_Tanh:{hyptan}_isSixteen:{sixteen}_Separable:{separable}_Group:{group}_Epoch{epoch+1}"')

  print(f"Best config {best_config} reached 12-margin accuracy of: {best_val_acc}")

# trying out with batch normalization, patience is significantly increased...
further_main(config_cnt=1, batchnorm=True)

# trying out with tanh and batchnorm since batchnorm improved, patience is significantly increased...
further_main(config_cnt=1, batchnorm=True, hyptan=True)

# trying out with 16 channels, without tanh and with batchnorm since only batchnorm improved, patience is significantly increased...
further_main(config_cnt=1, batchnorm=True, hyptan=False, sixteen=True)

# trying out with separable conv. layers, 16 channels, without tanh and with batchnorm since batchnorm & 16 improved, patience is significantly increased...
further_main(config_cnt=1, batchnorm=True, hyptan=False, sixteen=True, separable=True)

# trying out with group conv. layers, 16 channels, without tanh and with batchnorm since batchnorm & 16 improved, patience is significantly increased...
further_main(config_cnt=1, batchnorm=True, hyptan=False, sixteen=True, separable=False, group=True)

